{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyP3O9l8nAjUlRA72O4FAXWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/c2fel/ml-fs25-coding-challenge-3rdGenMCS/blob/main/pretrained_EfficientNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aqkOfAbFp7bO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import glob\n",
        "from torch.amp import autocast, GradScaler\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIAsg6-4qNCb",
        "outputId": "f07f3efc-f514-48e8-f205-919b6619c559"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rasterio\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.11/dist-packages (from rasterio) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from rasterio) (2025.4.26)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.11/dist-packages (from rasterio) (8.2.0)\n",
            "Collecting cligj>=0.5 (from rasterio)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from rasterio) (2.0.2)\n",
            "Collecting click-plugins (from rasterio)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from rasterio) (3.2.3)\n",
            "Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Installing collected packages: cligj, click-plugins, affine, rasterio\n",
            "Successfully installed affine-2.4.0 click-plugins-1.1.1 cligj-0.7.2 rasterio-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imVnXkTGqNp1",
        "outputId": "9001445d-673e-4278-a434-0b9606ec338f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://madm.dfki.de/files/sentinel/EuroSATallBands.zip --no-check-certificate\n",
        "!unzip -q \"/content/EuroSATallBands.zip\"\n",
        "\n",
        "train_dir = \"/content/ds/images/remote_sensing/otherDatasets/sentinel_2/tif\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TknLOgWVta7o",
        "outputId": "723d74e8-ead4-4420-a9c4-6dbaaf1e4e3a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-17 14:47:49--  https://madm.dfki.de/files/sentinel/EuroSATallBands.zip\n",
            "Resolving madm.dfki.de (madm.dfki.de)... 131.246.195.183\n",
            "Connecting to madm.dfki.de (madm.dfki.de)|131.246.195.183|:443... connected.\n",
            "WARNING: cannot verify madm.dfki.de's certificate, issued by ‘CN=GEANT OV RSA CA 4,O=GEANT Vereniging,C=NL’:\n",
            "  Unable to locally verify the issuer's authority.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2067725275 (1.9G) [application/zip]\n",
            "Saving to: ‘EuroSATallBands.zip’\n",
            "\n",
            "EuroSATallBands.zip 100%[===================>]   1.92G  19.2MB/s    in 1m 45s  \n",
            "\n",
            "2025-05-17 14:49:35 (18.7 MB/s) - ‘EuroSATallBands.zip’ saved [2067725275/2067725275]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip -q \"/content/drive/MyDrive/MCS/ML-Coding-Challenge/testset.zip\"\n",
        "\n",
        "test_dir = \"/content/testset\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWZB9B8_tlho",
        "outputId": "4211978f-fb4c-4265-e077-620c3b0956f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify dataset structure\n",
        "print(\"Training subfolders:\", sorted(os.listdir(train_dir)))\n",
        "train_samples = glob.glob(os.path.join(train_dir, \"*\", \"*.tif\"))\n",
        "print(f\"Found {len(train_samples)} training .tif files\")\n",
        "print(\"Test files (sample):\", sorted(os.listdir(test_dir))[:10])\n",
        "test_samples = glob.glob(os.path.join(test_dir, \"*.npy\"))\n",
        "print(f\"Found {len(test_samples)} test .npy files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-A-EM0vtP3k",
        "outputId": "fe8835a9-606c-489a-c640-d5e63cc1af2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training subfolders: ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
            "Found 27000 training .tif files\n",
            "Test files (sample): ['test_0.npy', 'test_1.npy', 'test_10.npy', 'test_100.npy', 'test_1000.npy', 'test_1001.npy', 'test_1002.npy', 'test_1003.npy', 'test_1004.npy', 'test_1005.npy']\n",
            "Found 4232 test .npy files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for Sentinel-2 .tif files\n",
        "class TIFDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(self.classes)}\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        for cls_name in self.classes:\n",
        "            cls_dir = os.path.join(root_dir, cls_name)\n",
        "            for img_path in glob.glob(os.path.join(cls_dir, \"*.tif\")):\n",
        "                self.images.append(img_path)\n",
        "                self.labels.append(self.class_to_idx[cls_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        with rasterio.open(img_path) as src:\n",
        "            img = src.read([8, 4, 3]).transpose(1, 2, 0)  # B8, B4, B3\n",
        "        img = img / 10000.0\n",
        "        img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Custom Dataset for unlabeled .npy files\n",
        "class UnlabeledNPYDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted(glob.glob(os.path.join(root_dir, \"*.npy\")))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        npy_path = self.images[idx]\n",
        "        img = np.load(npy_path)\n",
        "        if img.ndim == 3 and img.shape[2] > 3:\n",
        "            img = img[:, :, [7, 3, 2]] / 10000.0  # B8, B4, B3\n",
        "            img = (img * 255).clip(0, 255).astype(np.uint8)\n",
        "        elif img.ndim == 2:\n",
        "            img = np.stack([img] * 3, axis=2).astype(np.uint8)\n",
        "        img = Image.fromarray(img)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, npy_path\n",
        "\n",
        "# Compute dataset-specific mean/std\n",
        "def compute_mean_std(dataset):\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    n_samples = 0\n",
        "    for images, _ in loader:\n",
        "        batch_samples = images.size(0)\n",
        "        images = images.view(batch_samples, images.size(1), -1)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        n_samples += batch_samples\n",
        "    mean /= n_samples\n",
        "    std /= n_samples\n",
        "    return mean.tolist(), std.tolist()\n",
        "\n",
        "# Initialize dataset for mean/std computation\n",
        "temp_dataset = TIFDataset(train_dir, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
        "mean, std = compute_mean_std(temp_dataset)\n",
        "print(f\"Mean: {mean}, Std: {std}\")\n",
        "\n",
        "# Data transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = TIFDataset(train_dir, transform=train_transforms)\n",
        "test_dataset = UnlabeledNPYDataset(test_dir, transform=test_transforms)\n",
        "\n",
        "# Train/validation split\n",
        "indices = list(range(len(train_dataset)))\n",
        "train_idx, val_idx = train_test_split(indices, test_size=0.2, stratify=train_dataset.labels, random_state=42)\n",
        "train_subset = Subset(train_dataset, train_idx)\n",
        "val_subset = Subset(train_dataset, val_idx)\n",
        "train_loader = DataLoader(train_subset, batch_size=64, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_subset, batch_size=64, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAKrlfprvJkh",
        "outputId": "f7a767f7-5303-4c69-ec85-f59bfafebb96"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: [0.22839796543121338, 0.09292010217905045, 0.102447010576725], Std: [0.05111726373434067, 0.02677196078002453, 0.018013840541243553]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from torchvision.models import efficientnet_b4, EfficientNet_B4_Weights\n",
        "model = efficientnet_b4(weights=EfficientNet_B4_Weights.IMAGENET1K_V1)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.features[-4:].parameters():\n",
        "    param.requires_grad = True\n",
        "num_features = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(num_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam([\n",
        "    {\"params\": model.features[-4:].parameters(), \"lr\": 1e-4},\n",
        "    {\"params\": model.classifier.parameters(), \"lr\": 1e-3}\n",
        "])\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-Q8nPlBvNPI",
        "outputId": "8995ac78-19eb-4043-a6a7-efc8a51c3c48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b4_rwightman-23ab8bcd.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b4_rwightman-23ab8bcd.pth\n",
            "100%|██████████| 74.5M/74.5M [00:00<00:00, 190MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30):\n",
        "    scaler = GradScaler('cuda')  # Updated to torch.amp\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast('cuda'):  # Updated to torch.amp\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                with autocast('cuda'):  # Updated to torch.amp\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
        "        scheduler.step(epoch_loss)\n",
        "\n",
        "# Inference function\n",
        "def infer_model(model, test_loader, class_names):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, paths in test_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            with autocast('cuda'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "            for path, pred in zip(paths, preds.cpu().numpy()):\n",
        "                # get file name\n",
        "                test_id = os.path.basename(path).replace(\".npy\", \"\")\n",
        "                # get id from filename\n",
        "                number = int(test_id.split(\"_\")[-1])\n",
        "\n",
        "                predictions.append({\n",
        "                    \"test_id\": number,\n",
        "                    \"label\": class_names[pred]\n",
        "                })\n",
        "    pred_df = pd.DataFrame(predictions)\n",
        "\n",
        "    # Generate timestamp string\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"/content/drive/MyDrive/MCS/ML-Coding-Challenge/EfficientNet_predictions_{timestamp}.csv\"\n",
        "\n",
        "    pred_df.to_csv(filename, index=False)\n",
        "    print(f\"Saved: {filename}\")\n"
      ],
      "metadata": {
        "id": "9I9yuML-vP2y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and infer\n",
        "print(\"Starting training...\")\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=30)\n",
        "print(\"\\nPerforming inference on test set...\")\n",
        "infer_model(model, test_loader, train_dataset.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "1QioO3MHvSfM",
        "outputId": "4420e6ed-2f63-4257-a612-db79603da8d8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/30, Loss: 0.0562, Val Accuracy: 0.9820\n",
            "Epoch 2/30, Loss: 0.0515, Val Accuracy: 0.9867\n",
            "Epoch 3/30, Loss: 0.0484, Val Accuracy: 0.9839\n",
            "Epoch 4/30, Loss: 0.0428, Val Accuracy: 0.9852\n",
            "Epoch 5/30, Loss: 0.0394, Val Accuracy: 0.9863\n",
            "Epoch 6/30, Loss: 0.0331, Val Accuracy: 0.9872\n",
            "Epoch 7/30, Loss: 0.0321, Val Accuracy: 0.9848\n",
            "Epoch 8/30, Loss: 0.0257, Val Accuracy: 0.9870\n",
            "Epoch 9/30, Loss: 0.0279, Val Accuracy: 0.9874\n",
            "Epoch 10/30, Loss: 0.0242, Val Accuracy: 0.9878\n",
            "Epoch 11/30, Loss: 0.0222, Val Accuracy: 0.9883\n",
            "Epoch 12/30, Loss: 0.0234, Val Accuracy: 0.9881\n",
            "Epoch 13/30, Loss: 0.0213, Val Accuracy: 0.9878\n",
            "Epoch 14/30, Loss: 0.0178, Val Accuracy: 0.9891\n",
            "Epoch 15/30, Loss: 0.0166, Val Accuracy: 0.9874\n",
            "Epoch 16/30, Loss: 0.0179, Val Accuracy: 0.9893\n",
            "Epoch 17/30, Loss: 0.0144, Val Accuracy: 0.9896\n",
            "Epoch 18/30, Loss: 0.0126, Val Accuracy: 0.9885\n",
            "Epoch 19/30, Loss: 0.0137, Val Accuracy: 0.9893\n",
            "Epoch 20/30, Loss: 0.0120, Val Accuracy: 0.9909\n",
            "Epoch 21/30, Loss: 0.0106, Val Accuracy: 0.9891\n",
            "Epoch 22/30, Loss: 0.0121, Val Accuracy: 0.9896\n",
            "Epoch 23/30, Loss: 0.0093, Val Accuracy: 0.9891\n",
            "Epoch 24/30, Loss: 0.0116, Val Accuracy: 0.9904\n",
            "Epoch 25/30, Loss: 0.0104, Val Accuracy: 0.9887\n",
            "Epoch 26/30, Loss: 0.0116, Val Accuracy: 0.9896\n",
            "Epoch 27/30, Loss: 0.0111, Val Accuracy: 0.9885\n",
            "Epoch 28/30, Loss: 0.0078, Val Accuracy: 0.9907\n",
            "Epoch 29/30, Loss: 0.0071, Val Accuracy: 0.9896\n",
            "Epoch 30/30, Loss: 0.0077, Val Accuracy: 0.9885\n",
            "\n",
            "Performing inference on test set...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "autocast.__init__() missing 1 required positional argument: 'device_type'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-53a44577b5e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPerforming inference on test set...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0minfer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-0354e8156ed1>\u001b[0m in \u001b[0;36minfer_model\u001b[0;34m(model, test_loader, class_names)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: autocast.__init__() missing 1 required positional argument: 'device_type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "filename_pth = f\"/content/drive/MyDrive/MCS/ML-Coding-Challenge/EfficientNet_satellite_{timestamp}.pth\"\n",
        "torch.save(model.state_dict(), filename_pth)\n",
        "print(f\"Model saved to {filename_pth}\")"
      ],
      "metadata": {
        "id": "2cNrJhedvUrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}